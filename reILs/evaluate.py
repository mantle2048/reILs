import shelve
import argparse
import json
import numpy as np
import os
import os.path as osp
import torch

from pathlib import Path
from typing import List, Optional, Tuple, Union, Dict
from collections import defaultdict

from reILs.infrastructure.loggers import VideoRecorder
from reILs.infrastructure.execution import RolloutSaver, local_sample, synchronous_parallel_sample
from reILs.infrastructure.data import Batch

"yanked and modified from https://github.com/ray-project/ray/blob/130b7eeaba/rllib/evaluate.py"

EXAMPLE_USAGE = """
Example usage via executable:
    ./evaluate.py /tmp/ray/checkpoint_dir/checkpoint-0 --run DQN
    --env CartPole-v0 --steps 1000000 --out rollouts.pkl
Example usage w/o checkpoint (for testing purposes):
    ./evaluate.py --run PPO --env CartPole-v0 --episodes 500
"""

def create_parser():
    parser = argparse.ArgumentParser(
        formatter_class=argparse.RawDescriptionHelpFormatter,
        description="Roll out a reinforcement learning agent given a checkpoint model.",
    )

    parser.add_argument(
        "exp-dir",
        type=str,
        nargs='?',
        help="exp_dir from which to roll out.",
        )

    parser.add_argument(
        "--local-mode",
        action="store_true",
        help="Run ray in local mode for easier debugging."
        )
    parser.add_argument(
        "--render", action="store_true", help="Render the environment while rollouting"
        )
    parser.add_argument(
        "--steps",
        default=0,
        help="Number of timesteps to roll out. Rollout will also stop if "
        "`--episodes` limit is reached first. A value of 0 means no "
        "limitation on the number of timesteps run.",
        )
    parser.add_argument(
        "--episodes",
        default=2,
        help="Number of complete episodes to roll out. Rollout will also stop "
        "if `--steps` (timesteps) limit is reached first. A value of 0 means "
        "no limitation on the number of episodes run.",
        )
    parser.add_argument(
        "--save-info",
        action="store_true",
        help="Save the info field generated by the step() method, "
        "as well as the action, observations, rewards and done fields.",
        )
    parser.add_argument(
        "--track-progress",
        action="store_true",
        help="Write progress to a temporary file (updated "
        "after each episode). An output filename must be set using --out; "
        "the progress file will live in the same folder.",
    )
    return parser


def keep_going(steps, num_steps, episodes, num_episodes):
    """Determine whether we've collected enough data"""
    # If num_episodes is set, stop if limit reached.
    if num_episodes and episodes >= num_episodes:
        return False
    # If num_steps is set, stop if limit reached.
    elif num_steps and steps >= num_steps:
        return False
    # Otherwise, keep going.
    return True

def run(args, parser):
    # Load configuration from exp_dir.
    exp_dir = args.exp_dir
    if exp_dir:
        config_dir = osp.join(args.exp_dir, 'config.json')
        params_dir = osp.join(args.exp_dir, 'params.pkl')

    if not osp.exists(config_dir) or not osp.exists(params_dir):
        raise ValueError(
            f"Could not find params.pkl or config.json in exp_dir: {exp_dir}!"
            )
    else:
        with open(config_dir, 'r') as f:
            config = json.loads(f)
        agent_class = config['agent_class']
        agent = agent_class(
            env=config.get('env_name'),
            config=config.get('agent_config')
        )
        agent.resume(params=params_dir)

    ray.init(local_mode=args.local_mode)
    num_steps = int(args.steps)
    num_episodes = int(args.episodes)

    # Do the actual rollout.
    with RolloutSaver(
        args.exp_dir,
        num_steps=num_steps,
        num_episodes=num_episodes,
        track_progress=args.track_progress,
        save_info=args.save_info,
        render=args.render,
    ) as saver:
        evaluate(agent, num_episodes, num_steps, saver)

def evaluate(
    agent,
    num_episodes: int,
    num_steps: int=0,
    saver: RolloutSaver=None,
    render: bool=False,
) -> List[Batch]:
    if saver is None:
        saver = RolloutSaver()

    assert hasattr(agent, 'workers') \
        and isinstance(agent.workers, Workerset), \
        f'Agent: {agent} must have workers to evaluate.'

    # no saver, just evaluate the agent performance.
    if saver.is_invalid:
        eval_batch_list = synchronous_parallel_sample(
            worker_set=agent.workers,
            concat=False,
        )
        return eval_batch_list
    # with saver, save the rollout to drives
    else:
        local_worker = agent.workers.local_worker()
        env, policy = local_worker.env, local_worker.policy

    steps = 0
    episodes = 0
    eval_batch_list = []
    img_obs = []
    while keep_going(steps, num_steps, episodes, num_episodes):
        done, ep_rew, ep_len, step_list = False, 0.0, 0, []
        obs = env.reset()
        if render:
            img_obs = [env.render(mode='rgb_array'))]
        while not done and keep_going(steps, num_steps, episodes, num_episodes):
            act = policy.get_action(obs)
            next_obs, rew, done, info = env.step(act)
            ep_rew += rew
            step_return = Batch(
                obs=obs, act=act, next_obs=next_obs,
                rew=rew, done=done, info=info, img_obs=img_obs,
                eps_id=episodes, ep_len=ep_len, ep_rew=ep_rew,
            )
            if render:
                img_obs = [env.render(mode='rgb_array'))]
            step_list.append(step_return)
            obs = next_obs
            steps += 1
            ep_len += 1
        if done:
            episodes += 1

        batch = Batch.cat(step_list)
        saver.store(batch)
        eval_batch_list.append(batch)

    return eval_batch_list
